<!-- markdownlint-disable MD024 -->
<!--
  MD024 is disabled here because we intentionally repeat headers like 
  "Contributions", "Challenges" and "Progress" in each member's individual
  retrospectives. These duplicates are purposeful, so ignoring
  this rule prevents unnecessary lint warnings for valid cases.
-->
# Milestone 3: Data Analysis

**Milestone dates:** July 1 ‚Äì July 21

The main goal of this milestone was to identify and apply appropriate analysis  
techniques that matched our research questions, data characteristics, and  
team constraints. We focused on selecting methods that would effectively  
answer our research questions while being realistic about our technical  
capabilities and the nature of our dataset. This milestone also welcomed  
Muhammet Isik, who joined us from Group DECQ to contribute to our work.

Below is our collective reflection on what went well, what we can improve, and  
what we‚Äôve learned from working together during this phase.

---

## üî¥ Stop Doing

- Overestimating our available time windows given competing commitments and  
  coordination challenges.  
- Working on tasks in isolation without sharing progress early;  
  regular check-ins help us stay better aligned.

---

## üü¢ Continue Doing

- Maintaining realistic expectations about our technical capabilities and  
  choosing appropriate analysis methods accordingly.  
- Supporting each other through the learning curve of working with complex,  
  real-world datasets.  
- Maintaining open communication about challenges and blockers as they arise.

---

## üü° Start Doing

- Setting up more frequent check-ins to maintain momentum and keep everyone  
  aligned on progress.  
- Creating clearer task distribution early in the milestone to maximize  
  our collective efficiency.  
- Establishing better communication protocols to stay connected when working  
  asynchronously across different schedules.  
- Planning analysis phases with more realistic buffer time for unexpected  
  data challenges.

---

## üí° Lessons Learned

- Government datasets are authoritative, but also often large, messy, and  
  time-consuming to clean.  
- Real-world data analysis often involves more iterative problem-solving than  
  expected.  
- Choosing simpler techniques that align with our question and constraints  
  (e.g., chi-square tests, time series analysis, logistic regression) can be  
  more impactful than complex ones.  
- Flexibility is essential as the team navigates between gaps in availability,  
  skill levels, and time zones.  
- Having a range of experience levels has supported shared learning, even when  
  progress was uneven.

---

## üìä Strategy vs. Board

### 1Ô∏è‚É£ What parts of our plan went as expected?

- Successfully identified analysis techniques (chi-square, time series,  
  logistic regression) that are appropriate for our research questions and  
  skill level.  
- Maintained focus on matching analysis methods to our actual constraints  
  rather than pursuing overly complex approaches.  
- Found that our emphasis on simplicity and appropriateness over sophistication
  aligned well with the milestone objectives.

### 2Ô∏è‚É£ What parts of our plan did not work out?

- Coordination across different schedules and time zones proved more  
  challenging than anticipated, requiring us to adapt to more asynchronous  
  work patterns.  
- The comprehensive nature of our government dataset required more extensive  
  preprocessing work than initially planned.  
- We spent significant time during this milestone on cleaning tasks that we  
  had originally hoped to finish earlier.

### 3Ô∏è‚É£ Did you need to add things that weren't in your strategy?

- Enhanced our data cleaning and validation processes to properly handle the  
  characteristics of our comprehensive dataset.  
- Adapted to more asynchronous work patterns than originally planned due to  
  scheduling coordination needs.  
- Initially, we had not planned to use 'absentia' outcomes as a measure of  
  representation. However, since our first-selected tables did not include
  reliable legal representation data and the dataset was too large to explore
  everything in time,  
  we temporarily used 'absentia' data as a proxy to keep analysis moving.  
  Fortunately, our teammates later discovered and cleaned another table  
  containing actual legal representation information.

### 4Ô∏è‚É£ Or remove extra steps?

- Streamlined our approach to focus on core analysis techniques rather than  
  exploring every possible method.

---

## üë§ Individual Retrospectives

### Banu Ozyilmaz

### Contributions

- Explored the dataset structure and identified key variables from the `B_tblProceeding`
  table relevant to our research question  
- Worked on initial data analysis and drafted the first version of the notebook
- Maintained the 1_datasets documentation by recording newly added tables  
- Added content to the retrospective file, recorded agenda notes, and organized
  them in the notes folder

#### Challenges

- Handling the size and complexity of the dataset was very challenging for me
- Coordinating work across different time zones and personal commitments
  created scheduling difficulties
- Experienced a personal loss during this period that affected my availability
  and focus
- Learning new tools and methods while working under time pressure was demanding

#### Progress

- Gained hands-on experience with data manipulation tools like pandas
- Improved ability to identify meaningful patterns in complex datasets
- Learned to choose analysis methods that match available resources and time
- Developed better understanding of data limitations and uncertainty
- Built some confidence in working with real-world, messy data

### Member2
